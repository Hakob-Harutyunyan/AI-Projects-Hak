{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtZ9xva0MLOoHNaLQ338e5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpW0FUGQ1tHu",
        "outputId": "3004ca00-b43f-49df-a1c5-b568ccd60f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-nlp --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub0Ve5TxzYZ3",
        "outputId": "595e8dbe-63e6-4c78-b547-4562f009b0ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.6.2-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/590.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.1/590.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras-nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/950.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (23.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2023.6.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.5.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.8)\n",
            "Collecting tensorflow-text (from keras-nlp)\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras-core->keras-nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (2.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.57.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.2.2)\n",
            "Installing collected packages: namex, keras-core, tensorflow-text, keras-nlp\n",
            "Successfully installed keras-core-0.1.7 keras-nlp-0.6.2 namex-0.0.7 tensorflow-text-2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/Datasets/paragraphs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZWEyyGW1vCY",
        "outputId": "73924c5a-85aa-412c-9767-224c1b84063a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Datasets/paragraphs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from keras.layers.experimental import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras_nlp\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wbAZFGGG1wLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028e5137-5865-4823-af80-f6347e49df79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TXTDataset = \"paragraphs.txt\""
      ],
      "metadata": {
        "id": "wtThCnoV1xU3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TXTDataset,\"r\",encoding=\"utf-8\") as folder:\n",
        "  folder_content = folder.read()\n",
        "\n",
        "paragraphic = sorted(set(folder_content))\n",
        "len(paragraphic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbKzViDaPpjR",
        "outputId": "e44d4109-9750-4b65-ea62-560d5d581b31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TXTDataset, \"r\", encoding=\"utf-8\") as file:\n",
        "    file_contents = file.read()\n",
        "\n",
        "paragraphs = file_contents.split(\"\\n\")"
      ],
      "metadata": {
        "id": "dWSzCGyQ1zce"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myDataDataset = np.array(paragraphs)"
      ],
      "metadata": {
        "id": "4t4WLOVa12ym"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataLabeling(prelabeledData):\n",
        "  YLabels = []\n",
        "\n",
        "  for i in prelabeledData:\n",
        "    YLabels.append(float(\"{:.2f}\".format(len(i)/200)))\n",
        "\n",
        "  return YLabels"
      ],
      "metadata": {
        "id": "h5KULjXM15Ov"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YDataset = np.array(DataLabeling(myDataDataset))"
      ],
      "metadata": {
        "id": "fQ6eu5Jn16UO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testa = iter(YDataset)"
      ],
      "metadata": {
        "id": "kGgPsXnN17n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\n",
        "    \"bert_base_en_uncased\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7p3kEvvzfw-",
        "outputId": "8289c842-42eb-4a1e-a066-a498bb82b138"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/bert_base_en_uncased/v1/vocab.txt\n",
            "231508/231508 [==============================] - 0s 2us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = tokenizer(myDataDataset)"
      ],
      "metadata": {
        "id": "x7Cie6Zmztcm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "5jaYoywz1OQG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.num_words"
      ],
      "metadata": {
        "id": "USFMejis1F51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tf.strings.unicode_split(myDataDataset, input_encoding='UTF-8')"
      ],
      "metadata": {
        "id": "FRNoTKm8HVbC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asr = iter(chars)"
      ],
      "metadata": {
        "id": "xJWXtgZpIbgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(asr)"
      ],
      "metadata": {
        "id": "sQo4fLS9I0bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(paragraphic), mask_token=None)"
      ],
      "metadata": {
        "id": "b5z4OEwmI9Xj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids[0]"
      ],
      "metadata": {
        "id": "HfaxR9nDRt5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "8NubQvVGSBGB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids[0])\n",
        "chars"
      ],
      "metadata": {
        "id": "5j5A1qEoTtqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3_qTP7qT5qR",
        "outputId": "7f68c40b-8653-4bae-fccc-e2f60c4a240f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'This biographical article related to French artistic gymnastics is a stub. You can help Wikipedia by expanding it.'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "oWIHcjIyT9Kv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = ids_from_chars(tf.strings.unicode_split(np.array(paragraphs), 'UTF-8'))"
      ],
      "metadata": {
        "id": "HYxv1eVXUC65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b5ebed20-3035-417b-ee4f-e402cba5e3c2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-086907cc8908>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_from_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ids_from_chars' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(YDataset.shape,myDataDataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALmMNn05yd6C",
        "outputId": "bcb131ce-0e39-49d9-cf6f-cdd7a305079f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31613,) (31613,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((np.array([0,0,0,0,0,0,0,0]) == 0).all())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwEBT8TIjz25",
        "outputId": "afcbb0dc-507f-4b4b-b5e5-cdfa295e992d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((Dataset.to_tensor(), YDataset))"
      ],
      "metadata": {
        "id": "tnhuYGc919z-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(2):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "alEk3b0t1--e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "VPAWnzGa2A6m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(10000)\n",
        "    .batch(128, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH0DI0E02Com",
        "outputId": "dc283efe-ff2a-4771-b693-e7ea496a3c03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(128, 165), dtype=tf.int32, name=None), TensorSpec(shape=(128,), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This was the last code with GRU and OOP**"
      ],
      "metadata": {
        "id": "oFDquhx8TKUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.Embedding = keras.layers.Embedding(len(paragraphic), embedding_dim)\n",
        "    self.gru = keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.gru2 = keras.layers.GRU(64,return_sequences=False,\n",
        "                                   return_state=False)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.tokenizer = Tokenizer()\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    for XData,YData in enumerate(inputs):\n",
        "      input = XData\n",
        "      label = YData\n",
        "      # print(\"--------------------------------------------------\\n\",XData,YData , \"These are the data being given \\n\")\n",
        "      x = input\n",
        "      x = self.Embedding(x)\n",
        "      x = keras.layers.Reshape((1, -1))(x)\n",
        "      if states is None:\n",
        "        states = self.gru.get_initial_state(x)\n",
        "      x, states = self.gru(x, initial_state=states, training=training)\n",
        "      x, states = self.gru2(x)\n",
        "      x = keras.layers.Reshape((32,1))(x)\n",
        "      x = Dense(1,activation=\"relu\")(x)\n",
        "      x = Flatten()(x)\n",
        "\n",
        "    # x = inputs\n",
        "\n",
        "\n",
        "    # if return_state:\n",
        "    #   return x, states\n",
        "    # else:\n",
        "    #   return x\n",
        "\n",
        "    return x,states if return_state else x"
      ],
      "metadata": {
        "id": "mVGAyJbo2IEu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    for i,batch in enumerate(dataset.batch(batch_size)):\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "              # print(X_batch.shape , \"THis is the shape of batch\")\n",
        "              predictions = model(X_batch , training=True)\n",
        "              # print(\"----------------------------------------------------\\n\\n\",predictions[0] , \" : This is the prediction being made \\n \\n --------------------------------\")\n",
        "              # print(np.array(predictions).shape , \"This is the predictions shape\")\n",
        "              # print(y_batch)\n",
        "              print(np.array(y_batch).shape,\"\\n\",np.array(predictions).shape)\n",
        "              loss = model.loss(y_batch, predictions)\n",
        "\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Step {i}/{len(dataset)} - Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "bbymZbALXMTh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(paragraphic)"
      ],
      "metadata": {
        "id": "7Wcmdw3M2K2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **This is the normal model being created**"
      ],
      "metadata": {
        "id": "KOOalZl1TRJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "\n",
        "# model.add(keras.layers.Reshape((1, -1)))\n",
        "\n",
        "# model.add(tf.keras.layers.SimpleRNN(units=1024, return_sequences=False, return_state=False))\n",
        "# gru = tf.keras.layers.GRU(1024, return_sequences=True, return_state=True)\n",
        "# whole_sequence_output, final_state = gru(inputs)\n",
        "# model.add(Input(shape=(1)))\n",
        "# model.add(Embedding(31139,512))\n",
        "model.add(Embedding(632508,512,input_shape=(None,)))\n",
        "model.add(Bidirectional(units=256,return_sequences=False,return_state=False))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(1, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "XHxt4f13TrEq",
        "outputId": "12e07fce-08bd-430e-e8cb-874f5c93f400"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fecc43e891ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.add(keras.layers.Reshape((1, -1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.003),loss=keras.losses.MeanSquaredLogarithmicError(),metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "stHGpS5Tm7gi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset,epochs=2)"
      ],
      "metadata": {
        "id": "tQYUojvgvZkH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "9ed14e0f-a87c-4e7d-f188-e2cf61465a0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a4a101a41082>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3873\u001b[0m         \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3875\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   3876\u001b[0m                 \u001b[0;34m\"You must compile your model before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3877\u001b[0m                 \u001b[0;34m\"training/testing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LoremIpsum_text = \"\"\"Lorem ipsum dolo\n",
        "r sit amet, consectetur adipiscing el\n",
        "it. Pellentesque quis hendrerit justo.\n",
        " In sed aliquam tellus. Ut auctor nisl sit amet dui cursus, quis hendrerit diam ultrices. Donec aliquet, sapien a cursus ullamcorper, eros velit conval\n",
        " lis tortor, eu vestibulum turpis dolor ac justo. Vivamus a volutpat mauris, in ultricies arcu. Vestibulum bibendum arcu non sapien fringilla, ac tempus purus vulputate. Suspendisse semper augue vitae odio consequat, ut\n",
        " volutpat dolor suscipit. Vestibulum elementum quam et tellus feugiat, in cursus tortor vehicula. Integer luctus, dolor eget aliquam aliquam, odio justo cursus urna, in cursus tellus est a felis. Vivamus sed euismod arcu. Prae\n",
        " sent sit amet lobortis leo, a facilisis dui. Ut rhoncus ut libero id tempor. Integer condimentum, odio eget ullamcorper cursus, urna nisi placerat nisl, vel dictum metus dui quis tortor.\"\"\"*5\n",
        "print(len(LoremIpsum_text))\n",
        "# Generate a 512-character paragraph\n",
        "generated_paragraph = LoremIpsum_text[:512]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd7SxB0sUnEi",
        "outputId": "97cacd9f-f040-42c5-dd9e-58b860787aed"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(generated_paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "t6f3cy_lVNR6",
        "outputId": "57698cf2-21fb-4f0c-9dd6-63b653f276c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2fc85875e17a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_paragraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    957\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.squeeze(np.array([[1,2],[2,512]])))"
      ],
      "metadata": {
        "id": "xwvelZWVPktJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b137127d-1e88-42fe-db8f-09f47b33ce40"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2]\n",
            " [  2 512]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "train_size = int(len(list(dataset)) * train_ratio)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "_tqOKlIMyTee",
        "outputId": "c12cb837-7a72-4940-ba84-927e91b76a84"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-46bf61c6dcaa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n\u001b[0m\u001b[1;32m    509\u001b[0m                          \"iteration in eager mode or within tf.function.\")\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.take(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZBpgiraoELD",
        "outputId": "6c9371bc-33f4-4a74-9d34-722c6377c0b4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TakeDataset element_spec=(RaggedTensorSpec(TensorShape([512, None]), tf.int32, 1, tf.int64), TensorSpec(shape=(512,), dtype=tf.float64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDsNMfztRZnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}